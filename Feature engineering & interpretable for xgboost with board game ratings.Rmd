---
title: "Feature engineering & interpretable for xgboost with board game ratings"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
rm(list = ls())
gc()

knitr::opts_chunk$set(cache=TRUE, cache.lazy = TRUE, warning = FALSE,
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5)
library(tidyverse)

theme_set(theme_light())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))

```


## Explore data

```{r}
ratings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv")
details <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv")

head(ratings)
head(details)
```

```{r}
ratings_joined <-
  ratings %>%
  left_join(details, by = "id")

head(ratings_joined)

```

```{r}
ggplot(ratings_joined, aes(average)) +
  geom_histogram(alpha = 0.8)

```

데이터에서 minage = NA 인 값을 제거하고 min_age를 4등분하여 범주화시킨다. 여기서 보면 minage는 0세 ~ 25세까지 분포하고 있고 NA값을 가진 데이터가 200개가 된다. 이 중 NA를 가진 데이터를 제외하고 cut_number로 4개 범주로 나이대를 구분(0~8세, 8~10세, 10~12세, 12~25세)한 다음 각 나이분포에 대한 평점을 박스로 그렸다.  

```{r}
ratings_joined %>% 
  filter(!is.na(minage)) %>% 
  mutate(minage = cut_number(minage, 4)) %>% 
  ggplot(aes(minage, average, fill = minage)) + 
  geom_boxplot(alpha = 0.2, show.legend = FALSE)


```


## Tune an xgboost model

select에서 필요한 열을 골라낼 때 matches()를 사용하여 min 혹은 max를 찾아내었음. 그 다음 na.omit()를 이용하여 NA를 포함하고 있는 행을 모두 삭제    

```{r}
library(tidymodels)

preprocess <- 
  ratings_joined %>% 
  select(name, average, matches("min|max"), boardgamecategory) %>% 
  na.omit()

preprocess

```

```{r}
game_split <- 
  preprocess %>% 
  initial_split(strata = average)

game_split

```


```{r}
game_train <- training(game_split)
game_test <- testing(game_split)

set.seed(234)
game_folds <- vfold_cv(game_train, strata = average)
game_folds


```

### Preprocessing

1. step_token에 'custom_token' 옵션을 사용하기 위해 split_category()함수를 만든다.  
  - str_split() 함수를 사용하여 콤마(,) 기준으로 단어를 나눈 후  
  - str_remove_all() 함수를 이용하여 정규표현식으로 구두점([[:punct:]])을 없앤다. 
  - str_squish() 함수를 사용하여 문자열 양 옆에 있는 공백 등을 제거. 중간에 있는 공백은 한 칸 짜리 공백 (' ') 으로 치환, 이 함수를 하용하지 않을 경우 아래 str_replace_all()로 공백을 언더스코어로 바꿀 때 언더스코어가 연달아 두 개가 붙는 경우가 발생한다.  
  - str_to_lower() 함수를 이용하여 단어들을 소문자로 나눈다.  
  - str_replace_all() 함수를 이용하여 공백을 언더스코어(_)로 변환한다.  
  - 주의!! list나 vector 안에 있는 각각의 엘리먼트에 함수를 적용하기 위해서는 map() 함수를 써야 한다.  

2. boardgamecategory를 tokenize한다. >> step_tokenize()  
  - custom_token()을 사용하여 토크나이즈 시킬 때마다 구두점을 없애고 소문자로 만들게 한다.  

3. 30개의 가장 일반적인 Category만 남긴다. >> step_tokenfilter()  
  - max_tokens = 정수, max_times 및 min_times로 필터링한 후 상위 max_tokens 토큰만 유지. 기본값은 100  
   
4. 각 토큰들을 개별 열로 바꾸어 카테고리에 해당하는지 여부를 확인 >> step_tf() 

5. 이렇게 recipe를 만들어서 bake()하여 확인하면 id속성으로 부여된 name을 제외하고는 모두 numeric으로 변경된 것을 확인할 수 있음.  

6. board_category는 step_tokenfilter()에서 max_tokens = 30으로 준 것처럼 상위 30개만 나온 것을 확인할 수 있고, 


```{r}
library(textrecipes)

# test for making function
# x1 <- game_train %>% sample_n(1) %>% pull(boardgamecategory)
# x1 %>% str_split(", ") %>% map(str_remove_all, "[[:punct:]]") %>% map(str_to_lower)


split_category <- function(x) {
  x %>% 
    str_split(", ", ) %>% 
    map(str_remove_all, "[[:punct:]]") %>% 
    map(str_squish) %>% 
    map(str_to_lower) %>% 
    map(str_replace_all, " ", "_")
}

game_rec <- 
  recipe(average ~ ., data = game_train) %>% 
  update_role(name, new_role = "id") %>% 
  step_tokenize(boardgamecategory, custom_token = split_category) %>% 
  step_tokenfilter(boardgamecategory, max_tokens = 30) %>% 
  step_tf(boardgamecategory)

game_prep <- prep(game_rec)
bake(game_prep, new_data = NULL) %>% str()

```


```{r}
xgb_spec <- 
  boost_tree(
    trees = tune(),
    mtry = tune(),
    min_n = tune(),
    learn_rate = 0.01
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

xgb_spec

```

```{r}
xgb_wf <- workflow(game_rec, xgb_spec)

xgb_wf
```

hyperparamter가 있기 때문에 빠르게 찾기 위해 racing approach를 사용하기 위해 `finetune`package를 사용   
finetune::tune_race_anova는 계속해서 에러가 발생. 동일한 현상이 블로그를 보니 다수에게 발생함.
패키지를 모두 업데이트하고 control에 pkgs = c("stringr")을 했는데도 계속 발생.  
일단 tune_grid로 돌렸음.  

```{r}
library(finetune)
doParallel::registerDoParallel()

set.seed(234)
xgb_game_rs <- 
  tune_grid(
    xgb_wf,
    game_folds,
    grid = 20,
    control = control_grid(verbose = TRUE, pkgs = c("stringr"))
  )

xgb_game_rs

```

## Evaluate model

```{r}
show_best(xgb_game_rs)

```


```{r}
select_best(xgb_game_rs, "rmse")

```

hyperparameter를 튜닝했고, 그 결과를 아직 workflow에 반영하지 않았음. 여기까지는 죽 돌린 것이며 거기서 가장 좋은 값이 위의 값임. 이 값이 나오는 mtry, trees, min_n값을 workflow에 반영하여야 함.  

last_fit()을 이용하여 test set으로 evaluation을 하여 fitting함. 여기까지 test set을 쓰지 않았음.

```{r}
xgb_last <- 
  xgb_wf %>% 
  finalize_workflow(select_best(xgb_game_rs, "rmse")) %>% 
  last_fit(game_split)

xgb_last

```

여기까지 한 후 아래에서 볼 수 있는 .estimate값이 test set에 대한 evaluation 결과임. (training set에 대한 값이 아님.)   

test set의 rmse값이 training set의 rmse값보다 나쁘게 나왔음.

```{r}
xgb_last %>% collect_metrics()

```


```{r}
library(vip)

extract_workflow(xgb_last)

```

```{r}
xgb_fit <- extract_fit_parsnip(xgb_last)

class(xgb_fit)

```


```{r}
vip(xgb_fit, geom = "point", num_feature = 12)

```

X_train에 matrix가 들어가야 하기 때문에 bake()에 composition 옵션을 이용하여 "matrix" 형태로 받음  

```{r}
library(SHAPforxgboost)

game_shap <- 
  shap.prep(
    xgb_model = extract_fit_engine(xgb_fit),
    X_train = bake(game_prep, 
                   has_role("predictor"), 
                   new_data = NULL, 
                   composition = "matrix")
  )

game_shap

```


```{r}
shap.plot.summary(game_shap)

```


```{r}
shap.plot.dependence(
  game_shap,
  x = "minage",
  color_feature = "minplayers",
  size0 = 1.2,
  smooth = FALSE, add_hist = TRUE
)

```




